{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "from glob import glob\r\n",
    "import json, os\r\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('D:/압축/ai_data.csv', index_col = 'file_name')\r\n",
    "new_df.drop('type',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "boxcorners    0\nansize        0\nclass         0\ndtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 컬럼별 고유값 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1920, 1440]    786523\n[4000, 3000]     29940\n[1, 1]            6247\n[1920, 1743]       614\n[1849, 1920]       523\n                 ...  \n[1920, 1720]         1\n[1920, 1417]         1\n[1920, 1004]         1\n[1920, 945]          1\n[1920, 1653]         1\nName: ansize, Length: 81, dtype: int64"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "스킨로션          799\n선케어          1217\n립케어블러셔       1218\n향수           1696\n팩마스크         2301\n여행용가방        2577\n베이스메이크업      2610\n안경테          2956\n우산양산         3453\n클렌징필링        3989\n에센스앰플        4352\n에어쿠션팩트       4396\n운동용가방        4705\n기타시계         6374\n남성지갑         6686\n여성지갑         7627\n패션시계        11362\n남성가방        11687\n아동샌들        13104\n아동모자        15337\n아동외출여행가방    15968\n아동운동화       18110\n기능화         18135\n남성워커부츠      18211\n선글라스        18227\n남성정장화       18940\n아동구두        19325\n기타지갑        19573\n아동책가방       27026\n여성샌들        30605\n아쿠아슈즈쪼리     31810\n실내화슬리퍼      38236\n운동화         38302\n패션모자        38938\n여성단화        41397\n남성캐주얼화      42179\n힐펌프스        47285\n여성부츠워커      50976\n캐쥬얼가방       54022\n여성가방        62450\n스니커즈슬립온     72519\nName: class, dtype: int64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(new_df['ansize'].value_counts())\r\n",
    "display(new_df['class'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 해상도 1920, 1440만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list =[]\r\n",
    "for i in range(len(new_df)):\r\n",
    "    if new_df.iloc[i]['ansize'] == '[1920, 1440]':\r\n",
    "        new_list.append(new_df.iloc[i])\r\n",
    "aa = pd.DataFrame(new_list)\r\n",
    "zz = []\r\n",
    "for i in aa.index:\r\n",
    "    zz.append(i.replace('.JPG','.jpg'))\r\n",
    "aa.reset_index(inplace=True)\r\n",
    "aa['filename'] = zz\r\n",
    "aa.drop('index',axis=1,inplace=True)\r\n",
    "aa.to_csv('E:/1920_1440.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\r\n",
    "# for i in aa['filename']:\r\n",
    "#     X.append(i)\r\n",
    "for i in glob('D:/ai_set/*.jpg'):\r\n",
    "    X.append(i)\r\n",
    "\r\n",
    "train_ratio = 0.75\r\n",
    "validation_ratio = 0.15\r\n",
    "test_ratio = 0.10\r\n",
    "\r\n",
    "\r\n",
    "# train is now 75% of the entire data set\r\n",
    "# the _junk suffix means that we drop that variable completely\r\n",
    "x_train, x_test = train_test_split(X, test_size=1 - train_ratio)\r\n",
    "\r\n",
    "# test is now 10% of the initial data set\r\n",
    "# validation is now 15% of the initial data set\r\n",
    "x_val, x_test = train_test_split(x_test, test_size=test_ratio/(test_ratio + validation_ratio)) \r\n",
    "\r\n",
    "for i in x_train:\r\n",
    "    with open('C:/Users/HumanForest/Desktop/darknet/build/darknet/x64/data/ai_product/train.txt','a', encoding='utf-8') as f:\r\n",
    "            f.write(i+'\\n')\r\n",
    "            f.close()\r\n",
    "for i in x_val:\r\n",
    "    with open('C:/Users/HumanForest/Desktop/darknet/build/darknet/x64/data/ai_product/val.txt','a', encoding='utf-8') as f:\r\n",
    "            f.write(i+'\\n')\r\n",
    "            f.close()\r\n",
    "for i in x_test:\r\n",
    "    with open('C:/Users/HumanForest/Desktop/darknet/build/darknet/x64/data/ai_product/test.txt','a', encoding='utf-8') as f:\r\n",
    "            f.write(i+'\\n')\r\n",
    "            f.close()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Json(TF_Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\r\n",
    "aa = []\r\n",
    "bb = []\r\n",
    "with open('D:/img_list.txt',encoding='utf-8') as f:\r\n",
    "    for i in f:\r\n",
    "        aa.append(i.split('\\n')[0])\r\n",
    "with open('D:/img_list1.txt',encoding='utf-8') as f:\r\n",
    "    for i in f:\r\n",
    "        bb.append(i.split('\\n')[0])\r\n",
    "print(len(aa), len(bb))\r\n",
    "change_dict = dict(zip(bb, aa))\r\n",
    "from glob import glob \r\n",
    "import os, json, shutil\r\n",
    "\r\n",
    "class_dict = {}\r\n",
    "with open('C:/Users/HumanForest/Desktop/darknet/build/darknet/x64/classes.txt', encoding='utf-8') as aa :\r\n",
    "    for i,j in enumerate(aa):\r\n",
    "        class_dict[j.split('\\n')[0]] = i \r\n",
    "aa = []\r\n",
    "for i in glob('D:/ai_set/*.txt'):\r\n",
    "    aa.append(i.split('\\\\')[-1].split('.')[0])\r\n",
    "\r\n",
    "for i in glob('D:/압축/**/*.jpg', recursive=True):\r\n",
    "    file_name = i.split('\\\\')[-1]\r\n",
    "    try:\r\n",
    "        if i.split('\\\\')[-1].split('.')[0] in aa:\r\n",
    "            shutil.copy(i, 'E:/ai_product/{}'.format(file_name))\r\n",
    "            with open(i.replace('.jpg','.json'), encoding='utf-8') as json_file:\r\n",
    "                json_data = json.load(json_file)\r\n",
    "                ann = json_data['regions'][0]\r\n",
    "                class_id = str(class_dict[ann['class']])\r\n",
    "                x1, y1, x2, y2 = str(ann['boxcorners'][0])+',', str(ann['boxcorners'][1])+',', str(ann['boxcorners'][2])+',', str(ann['boxcorners'][3])+','\r\n",
    "                with open('E:/train_set.txt', 'a', encoding='utf-8') as F:\r\n",
    "                    F.write('E:/ai_product/{}'.format(file_name) + ' ' + x1 + y1 + x2 + y2 + class_id +'\\n')\r\n",
    "                    F.close()\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex)\r\n",
    "        print(i)\r\n",
    "         \r\n",
    "if 'HF020171_0133_0027.jpg' in change_dict.keys(): \r\n",
    "    print(True)\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Json(darknet_Version) ** Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n",
      "D:/압축\\folder\\05_상품\\HF020066_상품_신발_슬리퍼기능화_실내화슬리퍼_지압슬리퍼\\HF020066_0104_1462.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\r\n",
    "import json\r\n",
    "from glob import glob\r\n",
    "\r\n",
    "test_set = []\r\n",
    "train_set = []\r\n",
    "with open('D:/압축/test_img.txt', encoding='utf-8') as f:\r\n",
    "    for i in f:\r\n",
    "        test_set.append(i.replace('.jpg','.json').split('\\n')[0])\r\n",
    "with open('D:/압축/train_img.txt', encoding='utf-8') as f:\r\n",
    "    for i in f:\r\n",
    "        train_set.append(i.replace('.jpg','.json').split('\\n')[0])\r\n",
    "\r\n",
    "class_dict = {}\r\n",
    "with open('C:/Users/HumanForest/Desktop/darknet/build/darknet/x64/classes.txt', encoding='utf-8') as aa :\r\n",
    "    for i,j in enumerate(aa):\r\n",
    "        class_dict[j.split('\\n')[0]] = i \r\n",
    "\r\n",
    "for j_file in glob('D:/압축/**/*.json', recursive = True):    \r\n",
    "    try:\r\n",
    "        if j_file in test_set or train_set:\r\n",
    "            with open(j_file, encoding='utf-8') as json_file:\r\n",
    "                jsonData = json.load(json_file)\r\n",
    "                img_x, img_y = jsonData['image']['imsize'][0], jsonData['image']['imsize'][1]  # Width, High of Image\r\n",
    "                ann = jsonData['regions']\r\n",
    "\r\n",
    "                check_set = set()\r\n",
    "                for i in range(len(ann)):\r\n",
    "                    if 'boxcorners' in ann[i]:\r\n",
    "                        bb = ann[i]['boxcorners']\r\n",
    "                        class_id = class_dict[ann[i]['class']]\r\n",
    "\r\n",
    "                        x = round(abs(bb[0] + bb[2]) / 2 / img_x, 6)\r\n",
    "                        y = round(abs(bb[1] + bb[3]) / 2 / img_y, 6)\r\n",
    "                        w = round(abs(bb[2] - bb[0]) / img_x, 6)\r\n",
    "                        h = round(abs(bb[3] - bb[1]) / img_y, 6)\r\n",
    "                        content = str(class_id) + ' ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h) # Yolov4 data set\r\n",
    "\r\n",
    "                        if j_file in check_set:\r\n",
    "                            # Append to file files\r\n",
    "                            file = open(os.path.join('D:/ai_set', j_file.replace('.json','.txt').split('\\\\')[-1]), 'a', encoding='utf-8')\r\n",
    "                            file.write('\\n')\r\n",
    "                            file.write(content)\r\n",
    "                            file.close()\r\n",
    "                        elif j_file not in check_set:\r\n",
    "                            check_set.add(j_file)\r\n",
    "                            # Write files\r\n",
    "                            file = open(os.path.join('D:/ai_set', j_file.replace('.json','.txt').split('\\\\')[-1]), 'w', encoding='utf-8')\r\n",
    "                            file.write(content)\r\n",
    "                            file.close()\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex)\r\n",
    "        print(j_file)\r\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cf8ccfc9052cc6350e28866b5fca894e7ce9d94bc8cc5e5cc02f50451db4f3fa"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}